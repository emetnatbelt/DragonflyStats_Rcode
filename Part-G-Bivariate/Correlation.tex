R Statistical Programming - Simple Linear Regression
 
Simple Linear Regression
We use the method of least squares
A Scatterplot is created and can initially be used to get an idea
about the nature of the relationship between the variables, e.g. if the
relationship is linear, curvilinear, or no relationship exists.
We read the data from the film SLRex1.txt into a data frame SLR1.
Correlation Coefficient
r =Cov(x,y)Var(x)Var(y)
Hypothesis test of the Correlation Coefficient
We can carry out an hypothesis test for the correlation coefficient
using the cor.test function.
Model Fitting
Model Diagnostics
 
A QQplot checks the assumption of normality.
 


\subsection{Correlation Structure Example}


\newpage
\section{Correlation and Simple Regression Models}

\subsection{Correlation}

A correlation coefficient is a number between -1 and 1 which measures the degree to which two variables are linearly related. If there is perfect linear relationship with positive slope between the two variables, we have a correlation coefficient of 1; if there is positive correlation, whenever one variable has a high (low) value, so does the other.

If there is a perfect linear relationship with negative slope between the two variables, we have a correlation coefficient of -1; if there is negative correlation, whenever one variable has a high (low) value, the other has a low (high) value.
A correlation coefficient of 0 means that there is no linear relationship between the variables.

We can determine the Pearson Correlation coefficient in R using the \texttt{cor()} command.
To get a more complete statistical analysis, with formal tests, we can use the command \texttt{cor.test()}
The interpretation of the output from the cor.test()procedure is very similar to procedures we have already encountered. The null hypothesis is that the correlation coefficient is equal to zero. This is equivalent to saying that there is no linear relationship between variables.


\begin{framed}
\begin{verbatim}
C=c(0,2,4,6,8,10,12) 
F=c(2.1,5.0,9.0,12.6,17.3,21.0,24.7)
cor.test(C,F)
\end{verbatim}
\end{framed}
\begin{verbatim}

        Pearson's product-moment correlation

data:  C and F 
t = 47.1967, df = 5, p-value = 8.066e-08
alternative hypothesis: true correlation is not equal to 0 
95 percent confidence interval:
 0.9920730 0.9998421 
sample estimates:
      cor 
0.9988796 
\end{verbatim}


\subsection{Spearman and Kendall Correlation}
Spearman and Kendall correlations are both \textbf{\emph{rank correlations}}. 
To implement Spearman and Kendall correlation, simply specify the type in the \texttt{method=" "} argument.
\begin{verbatim}
> cor(G,D)
[1] 0.3167869
>
> cor(G,D,method="spearman")
[1] 0.1785714
>
> cor(G,D,method="kendall")
[1] 0.1428571
> 
\end{verbatim}
The interpretation is very similar, but there are no confidence intervals for the estimates.

\subsection{Fitting a Regression Model}
A regression model is fitted using the \texttt{lm()} command.

Consider the response variable $F$ and predictor variable $C$.
\begin{framed}
\begin{verbatim}
C=c(0,2,4,6,8,10,12) 
F=c(2.1,5.0,9.0,12.6,17.3,21.0,24.7)
Fit1=lm(F~C)
\end{verbatim}
\end{framed}


\subsection{Confidence and Prediction Intervals for Fitted Values} 

Recall that a fitted value $\hat{Y}$ is a estimate for the response variable, as determined by a linear model. The difference between the observed value and the corresponding fitted value is known as the residual.

The \textbf{\emph{residual standard error}} is the conditional standard deviation of the dependent variable Y given a value of the independent variable X. The calculation of this standard error follows from the definition of the residuals.

The residual standard error is often called the root mean square error (RMSE), and is a measure of the differences between values predicted by a model or an estimator and the values actually observed from the thing being modelled or estimated.

Since the residual standard error is a good measure of accuracy, it is ideal if it is small.

\subsubsection{Prediction Intervals}
In contrast to a confidence interval, which is concerned with estimating a population parameter, a prediction interval is concerned with estimating an individual value and is therefore a type of probability interval. 

The complete standard error for a prediction interval is called the standard error of forecast, and it includes the uncertainty associated with the vertical “scatter” about the regression line plus the uncertainty associated with the position of the regression line value itself.





Part 2 Strategies for Data project
\begin{itemize}
\item Exploratory Data Analysis

The first part of your report should contain some descriptive statistics and summary values. Also include some tests for normality.

\item{Regression}
You should have a data set with multiple columns, suitable for regression analysis.
Familiarize yourself with the data, and decide which variable is the dependent variable.

Also determine the independent variables that you will use as part of your analysis.

\item{Correlation Analysis}
Compute the Pearson correlation for the dependent variable with the respective independent variables.  As part of your report, mention the confidence interval for the correlation estimate
Choose the independent variables with the highest correlation as your candidate variables.
For these independent variables, perform a series of simple linear regression procedures.
\begin{verbatim}
lm(y~x1)
lm(y~x2)
\end{verbatim}
Comment on the slope and intercept estimates and their respective p-values. Also comment on the coefficient of determination (multiple R squared). Remember to write the regression equations.
Perform a series of multiple linear regressions, using pairs of candidate independent variables.
\begin{verbatim}
lm(y~x1 +x2)
lm(y~x2 +x3)
\end{verbatim}
Again, comment on the slope and intercept estimates, and their respective p-values.
In this instance, compare each of the models using the coefficient of determinations. Which model explains the data best?
\subsection{Analysis of residuals}
Perform an analysis of regression residuals ( you can pick the best regression model from last section).
Are the residuals normally distributed?
	Histogram /  Boxplot / QQ plot / Shapiro Wilk Test
Also you can plot the residuals to check that there is constant variance.
\begin{verbatim}
y=rnorm(10)
x=rnorm(10)
fit1=lm(y~x)
res.fit1 = resid(fit1)
plot(res.fit1)
\end{verbatim}

