\chapter{Statistical Inference}

\section{Confidence Interval examples}

\subsection{Example}
A random sample of 15 observations is taken from a normally distributed population
of values. The sample mean is 94.2 and the sample variance is 24.86.
Calculate a 99\% confidence interval for the population mean.


\subsubsection{Solution}
$t_(14,0.005) = 2.977$
99\% CI is $94.2 \pm 2.977 \sqrt{24.86/15}$ \\i.e. $94.2 \pm 3.83$ \\i.e. $(90.37,98.03)$


\subsection{Example 1: paired T test}


\begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
X & 5.20 & 5.15 & 5.17 & 5.16 & 5.19 & 5.15\\
Y & 5.20 & 5.15 & 5.17 & 5.16 & 5.19 & 5.15\\
  \hline
\end{tabular}


\subsection{Example 2}

Seven measurements of the pH of a buffer solution gave the
following results:

\begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
5.12 & 5.20 & 5.15 & 5.17 & 5.16 & 5.19 & 5.15\\
  \hline
\end{tabular}

Task 1: Calculate the 95\% confidence limits for the true pH
utilizing $R$.


Solution. We are using Student t distribution with six degrees of
freedom and the following code gives us the confidence interval
for this problem.
\begin{verbatim}
>x <- c(5.12, 5.20, 5.15, 5.17, 5.16, 5.19, 5.15)
>n =length(x)
>alpha =0.05
>stderr =sd(x)/sqrt(n)
>LB=mean(x)+qt(alpha/2,6)* stderr
>UB=mean(x)+qt(1-alpha/2,6)* stderr
>LB
#[1] 5.137975
>UB
#[1]5.187739
\end{verbatim}


\subsubsection{example}
A survey of study habits wishes to determine whether the mean
study hours completed by women at a particular college is higher
than for men at the same college. A sample of $n_1$ = 10 women and
$n_2$ = 12 men were taken, with mean hours of study $\bar{x}_1$ =
120 and $\bar{x}_2$ = 105 respectively. The standard deviations
were known to be $\sigma_1$ = 28 and $\sigma_2$ = 35.

The hypothesis being tested is:

\begin{eqnarray}
H_{0}: \mu_1 = \mu_2\qquad \qquad (\mu_1 - \mu_2= 0)\\
H_{a}: \mu_1 \neq \mu_2 \qquad \qquad (\mu_1 - \mu_2 \neq 0)
\end{eqnarray}

In $R$, the test statistic is calculated using:

\begin{verbatim}
xbar1 <- 120
xbar2 <- 105
sd1 <- 28
sd2 <- 35
n1 <- 10
n2 <-12
TS <- ( (xbar1 - xbar2) - (0) )/sqrt( (sd1^2/n1) + (sd2^2/n2) )
TS
[1] 1.116536
\end{verbatim}
Now need to calculate the critical value or the p-value.


The critical value can be looked up using qnorm. Since this is a
one-tailed test and there is a > sign in $H_1$:

\begin{verbatim}
qnorm(0.95)
[1] 1.644854
\end{verbatim}

Since the test statistic is less than the critical value ( 1.116536 < 1:645 )there is not enough evidence to reject $H_0$
and conclude that the population mean hours study for women is
not higher than the population mean hours study for men.


The p-value is determined using pnorm.

Careful! Remember pnorm
gives the probability of getting a value LESS than the value specified. We want the probability of getting a value greater than
the test statistic.
\begin{verbatim}
1-pnorm(1.116536) # OR pnorm(1.116536, lower.tail=FALSE)
[1] 0.1320964
\end{verbatim}
\newpage




\subsection{Example}
Suppose that 9 bags of salt granules are selected from the supermarket
shelf at random and weighed. The weights in grams are 812.0, 786.7, 794.1,
791.6, 811.1, 797.4, 797.8, 800.8 and 793.2. Give a 95\% confidence interval for the
mean of all the bags on the shelf. Assume the population is normal.


Here we have a random sample of size n = 9. The mean is 798.30. The sample
variance is $s^2 = 72.76$, which gives a sample standard deviation $s = 8.53$.

The upper 2.5\% point of the Student's $t$ distribution with n-1 (= 9-1 = 8) degrees of freedom is 2.306.

The 95\% confidence interval is therefore from \\
$(798.30 - 2.306 \times (8.53/\sqrt{9}), 798.30 + 2.306 \times (8.53/\sqrt{9})$\\
which is\\
$(798.30 - 6.56, 798.30 + 6.56) = (791.74, 804.86)$\\
It is sometimes more useful to write this as $798.30 \pm 6.56$.

Note that even if we do not assume the population is normal (that assumption is
never really true) the Central Limit Theorem might suggest that the confidence interval
is nearly right. A larger confidence would increase the length of the interval, so we
trade off increased certainty of coverage against a longer interval.

\subsection{Example}
Ten soldiers visit the rifle range on two different weeks. The first
week their scores are:
67 24 57 55 63 54 56 68 33 43
The second week they score
70 38 58 58 56 67 68 77 42 38
Give a 95\% confidence interval for the improvement in scores from week one to
week two.


\subsubsection{Answer}


This is a case of paired samples, for the scores are repeated observations for each
soldier, and there is good reason to think that the soldiers will differ from each other
in their shooting skill. So we work with the individual differences between the scores.
We shall have to assume that the pairwise differences are a random sample from a
normal distribution.

The differences are:

3 14 1 3 -7 13 12 9 9 -5


Effectively we now have a single sample of size 10, and want a 95\% confidence
interval for the mean of the population from which these differences are drawn. For
this we use a Student's $t$ interval. The sample mean of the differences is 5.2, and
$s^2$ = 54.84. So $s = 7.41$, and the 95\% $t$ interval for the difference in the means is
$5.2 - 2.26(7.41)/\sqrt{10},  5.2 + 2.26(7.41)/\sqrt{10} = (.0.1, 10.5)$.

\subsection{Example} A sample of 50 households in one community
shows that 10 of them are watching a TV special on the national
economy. In a second community, 15 of a random sample of 50
households are watching the TV special. We test the hypothesis
that the overall proportion of viewers in the two communities does
not differ, using the 1 percent level of significance, as follows:

\subsection{2 sided test}
A two-sided test is used when we are concerned about a possible
deviation in either direction from the hypothesized value of the
mean. The formula used to establish the critical values of the
sample mean is similar to the formula for determining confidence
limits for estimating the population mean, except that the
hypothesized value of the population mean m0 is the reference
point rather than the sample mean.



\subsection{The $t$ distribution}
TESTING A HYPOTHESIS CONCERNING THE MEAN BY USE OF THE t
DISTRIBUTION:

The $t$ distribution is the appropriate basis for
determining the standardized test statistic when the sampling
distribution of the mean is normally distributed but $s$ is not
known. The sampling distribution can be assumed to be normal
either because the population is normal or because the sample is
large enough to invoke the central limit theorem. The $t$
distribution is required when the sample is small ($n < 30$). For
larger samples, normal approximation can be used. For the critical
value approach, the procedure is identical to that described in
Section 10.3 for the normal distribution, except for the use of $t$
instead of z as the test statistic.

\section{Confidence Interval} CONFIDENCE INTERVALS FOR THE MEAN\\
suppose that you wish to estimate the mean sales amount per
retail outlet for a particular consumer product during the past
year. The number of retail outlets is large. Determine the
95 percent confidence interval given that the sales amounts are
assumed to be normally distributed, $\bar{X} = $3,425, s = $200$ ,
and $n = 25.$\\ Ans. $3;346:60 to $3;503:40
\\
8.24. Referring to Problem 8.23, determine the 95 percent
confidence interval given that the population is assumed to be
normally distributed, $\bar{X} = $3,425, s = $200$ , and $n = 25.$
\\Ans. $3;342:44 to $3;507:56
\section{Two sample test}
Suppose one has two independent samples, x1, ..., xm and y1, ...,
yn, and wishes to test the hypothesis that the mean of the x
population is equal to the mean of the y population:

$H0 : \mu_{x} = \mu_{y}.$

Alternatively this can be formulated as $H0 : \mu_{x} - \mu_{y} =
0$.

Let $\bar{X}$ and $\bar{Y}$ denote the sample means of the xs and
ys and let $S_{x}$ and $S_{y}$ denote the respective standard
deviations. The standard test of this hypothesis $H_{0}$ is based
on the t statistic
\begin{equation}T = \frac{\bar{X} - \bar{Y} }{S_{p} \sqrt{1/m + 1/n} }
\end{equation}

where $S_{p}$ is the pooled standard deviation.

\begin{equation}
S_{p} = \sqrt{ \frac{(m-1)S^{2}_{x} +  (n-1)S^{2}_{y}}{m + n - 2}}
\end{equation}

Under the hypothesis $H_{0}$, the test statistic T has a t
distribution with $m + n - 2$ degrees of freedom when
\begin{itemize} \item both the xs and ys are independent random samples
from normal distributions \item the standard deviations of the x
and y populations, $\sigma_{x}$ and $\sigma_{y}$, are equal
\end{itemize}.

Suppose the level of significance of the test is set at $\alpha$.
Then one will reject H when $|T| < tn+m.2,\alpha/2$, where
$tdf,\alpha$ is the $(1 - \alpha)$ quantile of a t random variable
with df degrees of freedom.

If the underlying assumptions of

\subsection{Paired T test}
The mean and standard deviation of the sample d values are
obtained by use of the basic formulas in Chapters 3 and 4, except
that d is substituted for X.

The mean difference for a set of differences between paired
observations is $\bar{d} = \frac{\sum d_{i}}{n}$.

The deviations formula and the computational formula for the
standard deviation of the differences between paired observations
are, respectively,

\begin{eqnarray}
S_{d} = \sqrt{\frac{\sum (d_{i}-\bar{d})^2}{n-1}}\\
S_{d} = \sqrt{\frac{ \sum (d^2)- n(\bar{d}^2)}{n-1}}\\
\end{eqnarray}

The standard error of the mean difference between paired
observations is obtained for the standard error of the mean.
\subsubsection{Hypotheses}
\begin{eqnarray*}
H_{0}: \mu_{d} = 0\\
H_{1}: \mu_{d} \neq 0\\
\end{eqnarray*}

%----------------------------------------------------------------------------------------------------------------------SLR%
Statistics : Inference Procedures with R
Contents
Hypothesis Tests	1
Non-Parametric Tests	2
Logarithmic  Transformation	2
Grubbs’ Test	3
Testing the equality of variance.	3
Welch two sample t-test	3
Outliers	5
%---------------------------------------------------------------------------------------------%

Hypothesis Tests
As a programming language designed for Statistics, it is obvious that R will have a substantial capability for Statistical Analysis.
Firstly, let us look at how R implements well known statistical procedures.  Consider a data set X, comprised of the values {4,6,8,12,15,21}.

x = c(4,6,8,12,15,21)

mean(x)
sd(x)
var(x)
mad(x)


\subsection{Non-Parametric Tests}
Many statistical tests assume that you have sampled data from populations that follow a Normal distribution. 
Biological data never follow a Gaussian distribution precisely, because a Gaussian distribution extends infinitely in both directions, and so it includes both infinitely low negative numbers and infinitely high positive numbers. But many kinds of biological data follow a bell-shaped distribution that is approximately Gaussian. 
Because statistical tests work well even if the distribution is only approximately Gaussian (especially with large samples), these tests are used routinely in many fields of science.
An alternative approach does not assume that data follow a Gaussian distribution. These tests, called nonparametric tests, are appealing because they require fewer assumptions about the distribution of the data. In this approach, values are ranked from low to high, and the analyses are based on the distribution of ranks.
Often, the analysis will be one of a series of experiments. Since you want to analyze all the experiments the same way, you cannot rely on the results from a single normality test.
Logarithmic  Transformation

If data deviate substantially from a Gaussian distribution, using a nonparametric test is not the only alternative. Consider transforming the data to create a Gaussian distribution. Transforming to reciprocals or logarithms are often helpful.
Data can fail a normality test because of the presence of an outlier. Removing that outlier can restore normality.
The decision of whether to use a parametric or nonparametric test is most important with small data sets (since the power of nonparametric tests is so low). But with small data sets, normality tests have little power to detect non-normal distributions, so an automatic approach would give you false confidence.
With large data sets, normality tests can be too sensitive. A low P value from a normality test tells you that there is strong evidence that the data are not sampled from an ideal normal distribution. But you already know that, as almost no scientifically relevant variables form an ideal normal distribution. What you want to know is whether the distribution deviates enough from the normalideal to invalidate conventional statistical tests (that assume a Gaussian distribution). A normality test does not answer this question. With large data sets, trivial deviations from the idea can lead to a small P value.
 
Grubbs’ Test
Grubbs' test is a formal hypothesis test for assessing whether or not a  data set contains an outlier.
This data set is univariate and approximately normal distributed. This test is designed for assessing one outlier only.  If more outliers are suspected, alternative tests, such as the Tietjen-Moore test, are recommended.
Hypotheses: Grubbs' test is defined for the hypothesis: 
H0:  There are no outliers in the data set  
H1:  There is exactly one outlier in the data set  
install.packages(“outliers”)
library(outliers)
#Package Author : Lukasz Komsta (UMLUB, Poland)

grubbs.test(DAT002)

Author : Lukasz Komsta
install.packages(“outliers”)
library(outliers)

grubbs.test(DAT002)

Testing the equality of variance.
var.test(X,Y)
var.test(X,Z)

Welch two sample t-test
The Welch two sample t-test doesn’t not require the assumption that the two data sets under examination have equal variance.
The assumption of equal variance is testable using the “var.test” procedure.
t.test(X,Y)


Independent two sample t-tests
With    as the population mean for a data set, the null and alternative hypotheses are as follows:
 
 
This test requires that the two data sets have equal variance. This assumption must be specified when coding in R.
t.test(X,Y,var.equal=TRUE)


 





\section{Inference Procedures}
\subsection{Confidence Interval }
A confidence interval gives an estimated range of values which is likely to include an unknown population parameter, the estimated range being calculated from a given set of sample data. If independent samples are taken repeatedly from the same population, and a confidence interval calculated for each sample, then a certain percentage (confidence level) of the intervals will include the unknown population parameter. 

Confidence intervals are usually calculated so that this percentage is $95\%$, but we can produce $90\%$, $99\%$, $99.9\%$ (or whatever) confidence intervals for the unknown parameter. The width of the confidence interval gives us some idea about how uncertain we are about the unknown parameter. A very wide interval may indicate that more data should be collected before anything very definite can be said about the parameter.
\subsection{Power }
 The power of a statistical hypothesis test measures the test's ability to reject the null hypothesis when it is actually false - that is, to make a correct decision. In other words, the power of a hypothesis test is the probability of not committing a type II error. It is calculated by subtracting the probability of a type II error from 1, usually expressed as: 
\[\mbox{Power} = 1 - \mbox{P(type II error) } = 1- \beta \]The maximum power a test can have is 1, the minimum is 0. Ideally we want a test to have high power, close to 1.

\section{Single Sample Inference Procedures}
If we have a single sample we might want to answer several
questions:
\begin{itemize}
\item What is the mean value? \item Is the mean value
significantly different from current theory? (Hypothesis test)
\item What is the level of uncertainty associated with our
estimate of the mean value? (Confidence interval)
\end{itemize}

\begin{itemize}
\item (Last week : confidence interval for a mean) \item Revision:
For large samples ($n > 30$) and/or if the population standard
deviation ($\sigma$) is known, the usual test statistic is given
by: \[Z =\frac{\bar{X} - \mu}{SE(\bar{X})}\]

\item $S.E.(\bar{X}) = { \sigma \over \sqrt{n}} $ or ${s \over \sqrt{n}}$. 
\item For small samples, use the $t-$distribution with $n-1$ degrees of freedom.
\item Critical value from tables.
\item Compare test statistics and critical values.
\end{itemize}

To ensure that our analysis is correct we need to check for
outliers in the data (i.e. boxplots) and we also need to check
whether the data are normally distributed or not.

\begin{framed}
\begin{verbatim}
> t.test(X,mu=10)

        One Sample t-test

data:  X 
t = 14.1421, df = 4, p-value = 0.0001451
alternative hypothesis: true mean is not equal to 10 
95 percent confidence interval:
 10.08037 10.11963 
sample estimates:
mean of x 
     10.1 
\end{verbatim}
\end{framed}
