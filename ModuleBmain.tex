\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{fancyhdr}
\usepackage{subfiles}

%\usepackage{listings}
\usepackage{framed}
\usepackage{graphicx}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2570}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{LastRevised=Wednesday, February 23, 2011 13:24:34}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{Language=American English}

\pagestyle{fancy}
\setmarginsrb{20mm}{0mm}{20mm}{25mm}{12mm}{11mm}{0mm}{11mm}
\lhead{Module B} \rhead{Mr. Kevin O'Brien}
\chead{Statistical Analysis with \texttt{R}}
%\input{tcilatex}
\begin{document}

\tableofcontents

%MS4024 Inference procedures

In this section, we will look at various inference procedures ( confidence intervals and hypothesis tests that R can implement. We have already encountered the correlation test. 
As with every hypothesis test, there is a null hypothesis and an alternative hypothesis.
For each procedure, these will be formulated separately. The outcome of the test is either the rejection or the failure to reject the null hypothesis.

The key characteristics of the output of each test is the p-value. If the p-value is below a certain threshold ( we will use 1% or 0.01) we reject the null hypothesis, and accept the alternative hypothesis. Otherwise we fail to reject the null hypothesis.

It is also possible to implement one tailed tests (“greater than” or “less than tests”) by specifying an additional argument. You can use the alternative="less" or alternative="greater" option to specify a one tailed test. Access the help command for instructions on how to do this. 

For the sake of time we will solely use the two tailed test (tests of equality) for this 
module.

\subsection{Confidence intervals}
With most of these procedures, a confidence interval is specified automatically. We will interpret each confidence interval on a case by case basis.


%-----------------------------------------------
\subsection{Statistical Inference}
\begin{itemize}
\item $R$ commands for statistical inference procedures \item
t.test() - testing procedure for means.
\begin{itemize}
\item One sample \item Two sample \item Paired
\end{itemize}
\item prop.test() - testing procedure for proportions.
\begin{itemize}
\item One sample \item Two sample
\end{itemize}
\item var.test() - testing procedure for variances.
\end{itemize}
%-------------------------------%




%-----------------------------------------------
\subsection{Single sample inference}

If we have a single sample we might want to answer several
questions:
\begin{itemize}
\item What is the mean value? \item Is the mean value
significantly different from current theory? (Hypothesis test)
\item What is the level of uncertainty associated with our
estimate of the mean value? (Confidence interval)
\end{itemize}
To ensure that our analysis is correct we need to check for
outliers in the data (i.e. boxplots) and we also need to check
whether the data are normally distributed or not.
%-------------------------------%

%-----------------------------------------------
\subsection{Checking normality}


Graphical methods are often used to check that the data being
analysed are normally distributed. We can use
\begin{itemize}
\item Histogram - check for symmetry \item Boxplot - symmetry and
outliers \item Normal probability (Q-Q) plot

\item Other procedures
\begin{itemize}
\item Kolmogorov-Smirnov test (ks.test())\item Shapiro Wilk test (shapiro.test()) \item
Grubb's test \item Anderson Darling test
\end{itemize}
\end{itemize}
We shall revert to these tests later.
%-------------------------------%




%-----------------------------------------------
\subsection{Hypothesis testing for a mean}

\begin{itemize}
\item (Last week : confidence interval for a mean) \item Revision:
For large samples (n >= 30) and/or if the population standard
deviation ($\sigma$) is known, the usual test statistic is given
by: $Z =\frac{\bar{X} - \mu}{SE(\bar{X})}$

\item $SE(\bar{X}) = \sigma /\sqrt{n}$ or $s / \sqrt{n}$. \item For small samples, use the $t-$distribution
with $n-1$ degrees of freedom.
\item
Critical value from tables.
\item Compare test statistics and critical values.
\end{itemize}

%-------------------------------%


%-----------------------------------------------
\subsection{One sample t-test: t.test()}
\begin{itemize}
\item We can use the $R$ command `t.test()' to perform both procedures.
\item The t.test() command determines a confidence interval for the
mean of a data set, and performs a hypothesis test on the mean.
\item
The default null hypothesis is $H_{0}: \mu = 0$.
\item The default alternative hypothesis is $H_{1}: \mu \neq 0$.\\
\item The default confidence level is 95\% (hence default $\alpha$ = 0.05).
\item Base all decisions on p-values, i.e. reject the null if less than $\alpha$ or $\alpha/2$.
\end{itemize}

%-------------------------------%
%--------------------------------------------
\subsection{t-test}
\begin{verbatim}
> x <- c(3, 0, 5, 2, 5, 5, 5, 4, 4, 5)
> t.test(x)

        One Sample t-test

data:  x t = 7.125, df = 9, p-value = 5.514e-05
alternative hypothesis:
  true mean is not equal to 0
95 percent confidence interval:
 2.593516 5.006484
sample estimates: mean of x
      3.8

\end{verbatim}
%-------------------------------%
%----------------------------------------------------
\subsection{ one sample t-test: code}
We can give the output of the procedure a name (i.e. `test1').
The output is a data structure made up of several components.
These components can be found using the names() command, and accessed using the
'\$' symbol ( i.e. just like data frames )
\begin{verbatim}
> test1=t.test(x)
> names(test1)
[1] "statistic"  "parameter"   "p.value"
[4] "conf.int"  "estimate"    "null.value"
[7] "alternative" "method" "data.name"
\end{verbatim}
%-------------------------------%

\subsection{ one sample t-test: code}

\begin{verbatim}
> test1$method
[1] "One Sample t-test"
> test1$null.value
mean
   0
> test1$data.name
[1] "x"
>
> test1$conf.int                                                            $
[1]  2.593516 5.006484 attr
(,"conf.level")
[1] 0.95                                                                    $
> test1$p.value
[1] 5.514e-05
\end{verbatim}
%-------------------------------%
%-----------------------------------------------


\subsection[containsverbatim]{One sample t-test: Decision rule}
\begin{itemize}
\item The null hypothesis is $H_{0}: \mu = 0$.
\item The alternative hypothesis is $H_{1}: \mu \neq 0$.
\item The p-value is given in the output.
\begin{verbatim}
> test1$p.value
[1] 5.514e-05                                                           $
\end{verbatim}
\item This is a 2 tailed test, so compare p.value with $\alpha/2$.
\item The p-value is extremely small and is less than $\alpha/2$
\item Therefore we reject the null hypothesis. $\mu$ must be greater than 0.
\end{itemize}
%-------------------------------%
%----------------------------------------------
\subsection{One sample t-test: Null hypotheses}
We can specify the null hypothesis, e.g. $H_{0}: \mu = 3$. Consequently $H_{1}: \mu \neq 3$.
\begin{verbatim}
> t.test(x,mu=3)
....
....
t = 1.5, df = 9, p-value = 0.1679
alternative hypothesis:
  true mean is not equal to 3
95 percent confidence interval:
 2.593516 5.006484

\end{verbatim}
Note the p-value is now 0.1679. We fail to reject the null hypothesis.
The confidence interval is unaffected. Note that $\mu$(i.e 3) lies within the confidence interval.
%-------------------------------%
%-----------------------------------------------

\subsection{One sample t-test: significance}
We can also specify the confidence level $1-\alpha$ (and consequently $\alpha$).
\begin{verbatim}
> t.test(x,mu=3, conf.level=0.90)

        One Sample t-test

data:  x
t = 1.5, df = 9, p-value = 0.1679
alternative hypothesis:
   true mean is not equal to 3
90 percent confidence interval:
 2.82234 4.77766

\end{verbatim}
%-------------------------------%
\subsection{One sample t-test: significance}
\begin{itemize}

\item The p-value remains the same (it is independent of $\alpha$).
\item We fail to reject the null hypothesis.
\item A 90\% confidence interval is calculated.
\item The confidence interval validates the decision not to reject the null
hypothesis ( $\mu$ lies within the interval).
\end{itemize}
%-------------------------------%


%-----------------------------------------------

\subsection{The alternative hypothesis}
The default alternative hypothesis is the `two tailed' or `not equal' hypothesis.\\
Both the one tailed alternative hypotheses can be specified using the `alternative' parameter.
\begin{itemize}
\item alternative = "less"
\item alternative = "greater"
\end{itemize}

\begin{verbatim}
> t.test(x,mu=3, alternative ="less")
> t.test(x,mu=3, alternative ="greater")
\end{verbatim}

We shall test with both sets of hypotheses respectively.\\
1) $H_{0}: \mu \geq 3$ \quad v \quad $H_{1}: \mu < 3$\\
2) $H_{0}: \mu \leq 3$ \quad v \quad $H_{1}: \mu > 3$\\
%-------------------------------%
%-----------------------------------------------
\subsection{The 'less' alternative hypothesis}
\begin{verbatim}
> t.test(x,mu=3, alternative ="less")
.....
t = 1.5, df = 9, p-value = 0.916
alternative hypothesis:
    true mean is less than 3
95 percent confidence interval:
    -Inf 4.77766
sample estimates:
mean of x
      3.8
\end{verbatim}
Consider both the p-values and the confidence interval.
We fail to reject the null hypothesis.
%-------------------------------%
%--------------------------------------------
\subsection{ The 'greater' alternative hypothesis}


\begin{verbatim}
> t.test(x,mu=3, alternative ="greater")
.....
data:  x
t = 1.5, df = 9, p-value = 0.08393
alternative hypothesis:
    true mean is greater than 3
95 percent confidence interval:
 2.82234     Inf
sample estimates:
mean of x
      3.8
\end{verbatim}
Again we fail to reject the null hypothesis.
%-------------------------------%

%--------------------------------------------------
\subsection{Two Sample t-test}
\begin{itemize}
\item The two-sample t-test is one of the most commonly used
hypothesis tests. \item It is applied to compare whether the
average difference between two groups is really significant or if
it is due instead to random chance. \item It helps to answer
questions like whether the average success rate is higher after
implementing a new sales tool than before, \item or whether the
test results of patients who received a drug are better than test
results of those who received a placebo.
\item
$H_{0}: \mu_{x} = \mu_{y} \quad v \quad \mu_{x} \neq \mu_{y}$\\
\end{itemize}
%-------------------------------%

%----------------------------------------------------------
\subsection{Two Sample t-test}

To perform a two sample test, just name both data sets as arguments for the t.test()
command. \\ Both data sets dont necessarily have to be of equal length. \\ This test is called the Welch Two Sample t-test.
\begin{verbatim}
> x = c(3, 0, 5, 2, 5, 5, 5, 4, 4, 5)
> y = c(2, 1, 4, 1, 4, 3, 3, 2, 3, 5)
> t.test(x,y)
\end{verbatim}
%-------------------------------%
%-------------------------------------------------
\subsection{Two Sample t-test}
\begin{verbatim}
        Welch Two Sample t-test

data:  x and y
t = 1.478, df = 16.999, p-value = 0.1577
alternative hypothesis:
true difference in means is not equal to 0
95 percent confidence interval:
 -0.4274951  2.4274951
sample estimates:
mean of x mean of y
      3.8       2.8
\end{verbatim}
%-------------------------------%
%----------------------------------------------------------
\subsection{Two Sample t-test}
The p.value is 0.1577. Therefore we fail to reject the null hypothesis.\\
Also the null difference (0) lies within the confidence interval.\\

All of the variations described previously (i.e. specifying the null value, the confidence level and the alternative hypothesis)
apply to the two sample $t$-test also.

%-------------------------------%
%------------------------------------------------------------
\subsection{Paired t tests}
For `before/after' procedures, the corresponding elements of both data sets are said to be `paired'.
We can perform an analysis of the case-wise differences between both data sets.
Necessarily both data sets should be of the same length.
\begin{verbatim}
> x = c(3, 0, 5, 2, 5, 5, 5, 4, 4, 5)
> y = c(2, 1, 4, 1, 4, 3, 3, 2, 3, 5)
>
> x-y
 [1]  1 -1  1  1  1  2  2  2  1  0
> mean(x-y)
[1] 1
\end{verbatim}
Where the casewise differences are important, we add `paired=TRUE' as an argument to the t.test() command.
%-------------------------------%
%------------------------------------------
\subsection{Paired t tests}
\begin{verbatim}
> t.test(x,y,paired=TRUE)

        Paired t-test

data:  x and y t = 3.3541, df = 9,
 p-value = 0.008468
alternative hypothesis:
 true mean is not equal to 0
95 percent confidence interval:
 0.3255550 1.6744450
sample estimates: mean of x
                      1
> t.test(x,y,paired=TRUE)$p.value
[1] 0.00846815                                                               $
\end{verbatim}
%-------------------------------%
%------------------------------------------
\subsection{Paired t tests: alternative method}
The paired $t-$test is equivalent to a one sample $t-$test on the differences of $x$ and $y$, i.e. ($x-y$).
\begin{verbatim}
> t.test(x-y)
.....
data:  x - y t = 3.3541, df = 9,
 p-value = 0.008468
alternative hypothesis:
 true mean is not equal to 0
95 percent confidence interval:
 0.3255550 1.6744450
.....
> t.test(x-y)$p.value
[1] 0.00846815
\end{verbatim}
%-------------------------------%

\subsection{Paired t tests}
Both the p-value and the confidence interval validate the conclusion that we should reject the null hypothesis.
%-------------------------------%
%--------------------------------------------



\subsection{Hypothesis Tests for a Proportion }
\begin{itemize}
\item We can also perform a hypothesis test for a population
proportion, p. The $R$ function to carry out such a test is
prop.test.

\item Example: A manufacturer claims that the proportion of
defective items produced is approximately 4\%. A random sample of
size 50 is taken, 3 of which are defective. Is the manufacturer's
claim justified?
\end{itemize}
\begin{verbatim}
>prop.test(x=3, n=50, p = 0.04, alternative = "two.sided",
conf.level = 0.95)
\end{verbatim}
%-------------------------------%
\end{document}
%---------------------------------------------------
\subsection{Proportion test : Results}

\begin{verbatim}
        1-sample proportions test

data:  3 out of 50, null probability 0.04
X-squared = 0.1302, df =1, p-value = 0.7182
alternative hypothesis:
 true p is not equal to 0.04
95 percent confidence interval:
 0.01562459 0.17541874
sample estimates:
   p
0.06
\end{verbatim}
%-------------------------------%
%----------------------------------------
\subsection{prop.test}
Paramaters
\begin{itemize}
\item x   number of success ( value or vector)
\item n   sample size (value or vector)
\item p   Null hypothesis value is 50\%($ H_0: p = 0.5 $)
\item conf.level confidence level (default is 95\%)
\end{itemize}
%-------------------------------%
\subsection{prop.test
\begin{verbatim}

> prop.test(280,400)

    1-sample proportions test
    with continuity correction

data:  280 out of 400, null probability 0.5
X-squared = 63.2025, df = 1,
p-value = 1.865e-15
alternative hypothesis: true p is not equal to 0.5
95 percent confidence interval:
 0.6520722 0.7440176
sample estimates:
  p
0.7
\end{verbatim}


%-------------------------------%

%----------------------------------------------------------
\subsection{prop.test - 99\ confidence interval}
\begin{verbatim}
> prop.test(280,400,conf.level=0.99)

        1-sample proportions test

data:  280 out of 400, null probability 0.5
X-squared = 63.2025, df = 1, p-value = 1.865e-15
alternative hypothesis:
    true p is not equal to 0.5
99 percent confidence interval:
 0.6368118 0.7565247
sample estimates:
  p
0.7
\end{verbatim}



%---------------------------------------------------
\subsection{F test to compare two variances}
We can use the F test to test for equality in the variances,
provided that the two samples are from normal populations.

\begin{verbatim}
> var.test(x, y)

        F test to compare two variances

data:  A and B F = 2.8023, num df = 6, denom df = 4, p-value =
0.3378 alternative hypothesis: true ratio of variances is not
equal to 1 95 percent confidence interval:
  0.3046847 17.4502476
sample estimates: ratio of variances
           2.80228
\end{verbatim}

%-------------------------------%



\end{document}



